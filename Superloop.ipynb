{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superloop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq8FBNIQEWRuQ2AgR735wN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thuffy13/CNNs-For-DNA-Sequence-Analysis/blob/main/Superloop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw_NmMvH6PzS"
      },
      "outputs": [],
      "source": [
        "import os, h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow import keras\n",
        "import model_zoo\n",
        "import tfomics\n",
        "import tensorflow as tf\n",
        "\n",
        "filepath = 'truncated_deepsea_dataset.h5'\n",
        "dataset = h5py.File(filepath, 'r')\n",
        "\n",
        "with h5py.File(filepath, 'r') as dataset:\n",
        "    x_train = np.array(dataset['X_train']).astype(np.float32)\n",
        "    y_train = np.array(dataset['Y_train']).astype(np.float32)\n",
        "    x_valid = np.array(dataset['X_valid']).astype(np.float32)\n",
        "    y_valid = np.array(dataset['Y_valid']).astype(np.int32)\n",
        "    x_test = np.array(dataset['X_test']).astype(np.float32)\n",
        "    y_test = np.array(dataset['Y_test']).astype(np.int32)\n",
        "\n",
        "x_train = np.squeeze(x_train).transpose([0,2,1])\n",
        "x_valid = np.squeeze(x_valid).transpose([0,2,1])\n",
        "x_test = np.squeeze(x_test).transpose([0,2,1])\n",
        "\n",
        "N, L, A = x_train.shape\n",
        "num_labels = y_valid.shape[1]\n",
        "print(x_train.shape)\n",
        "\n",
        "pip install -U tensorflow-addons\n",
        "\n",
        "from tensorflow.python.tools import module_util\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "'''\n",
        "bn_before = True\n",
        "bn_after = False\n",
        "ln_before = False\n",
        "ln_after = False\n",
        "in_before = False\n",
        "in_after = False\n",
        "first_layer_pass = True\n",
        "'''\n",
        "\n",
        "keras.backend.clear_session() \n",
        "\n",
        "#input-shape = (L,A)\n",
        "def cnn4(input_shape, output_shape, multiplier, bn_before, bn_after, ln_before, ln_after, in_before, in_after, first_layer_pass):\n",
        "  # l2 regularization\n",
        "  l2 = keras.regularizers.l2(1e-6)\n",
        "\n",
        "  multiplier = 2;\n",
        "\n",
        "  # input layer\n",
        "  inputs = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # layer 1 - convolution\n",
        "  nn = keras.layers.Conv1D(filters=32*multiplier,\n",
        "                          kernel_size=19,\n",
        "                          strides=1,\n",
        "                          activation=None,\n",
        "                          use_bias=False,\n",
        "                          padding='same',\n",
        "                          kernel_regularizer=l2, \n",
        "                          )(inputs)\n",
        "  if (bn_before == True) and (first_layer_pass == False):        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_before == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_before == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "\n",
        "  nn = keras.layers.Activation('relu')(nn)\n",
        "\n",
        "  if bn_after == True:        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_after == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_after == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "  nn = keras.layers.MaxPool1D(pool_size=4)(nn)\n",
        "  nn = keras.layers.Dropout(0.1)(nn)\n",
        "\n",
        "  # layer 2 - convolution\n",
        "  nn = keras.layers.Conv1D(filters=128*multiplier,\n",
        "                          kernel_size=7,\n",
        "                          strides=1,\n",
        "                          activation=None,\n",
        "                          use_bias=False,\n",
        "                          padding='same',\n",
        "                          kernel_regularizer=l2, \n",
        "                          )(nn)        \n",
        "  if bn_before == True:        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_before == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_before == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "\n",
        "  nn = keras.layers.Activation('relu')(nn)\n",
        "\n",
        "  if bn_after == True:        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_after == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_after == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "  nn = keras.layers.MaxPool1D(pool_size=25)(nn)\n",
        "  nn = keras.layers.Dropout(0.1)(nn)\n",
        "\n",
        "  # layer 3 - Fully-connected \n",
        "  nn = keras.layers.Flatten()(nn)\n",
        "  nn = keras.layers.Dense(512*multiplier,\n",
        "                          activation=None,\n",
        "                          use_bias=False,\n",
        "                          kernel_regularizer=l2, \n",
        "                          )(nn)      \n",
        "  if bn_before == True:        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_before == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_before == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "\n",
        "  nn = keras.layers.Activation('relu')(nn)\n",
        "\n",
        "  if bn_after == True:        \n",
        "    nn = keras.layers.BatchNormalization()(nn)\n",
        "  elif ln_after == True:\n",
        "    nn = keras.layers.LayerNormalization()(nn)\n",
        "  elif in_after == True:\n",
        "    nn = tfa.layers.InstanceNormalization()(nn)\n",
        "  nn = keras.layers.Dropout(0.5)(nn)\n",
        "\n",
        "  # Output layer\n",
        "  logits = keras.layers.Dense(output_shape, activation='linear', use_bias=True,  \n",
        "                                  kernel_initializer='glorot_normal',\n",
        "                                  bias_initializer='zeros')(nn)\n",
        "  outputs = keras.layers.Activation('sigmoid')(logits)\n",
        "\n",
        "  # create keras model\n",
        "  return keras.Model(inputs=inputs, outputs=outputs)\n",
        "'''\n",
        "modely = cnn4((L,A), 12, 2, True,\n",
        "False,\n",
        "False,\n",
        "False,\n",
        "False,\n",
        "False,\n",
        "False)\n",
        "\n",
        "modely.summary()\n",
        "'''\n",
        "# save model weights\n",
        "#model.save_weights('my_model_weights.h5')\n",
        "\n",
        "'''\n",
        "# set up optimizer and metrics\n",
        "auroc = keras.metrics.AUC(curve='ROC', name='auroc')\n",
        "aupr = keras.metrics.AUC(curve='PR', name='aupr')\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0)\n",
        "modely.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=[auroc, aupr])\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import pickle\n",
        "from tfomics import moana, impress\n",
        "from pandas import DataFrame\n",
        "from tfomics import evaluate\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "norms = [[True, False, False, False, False, False, True], [True, False, False, False, False, False, False],\n",
        "         [False, True, False, False, False, False, True], [False, True, False, False, False, False, False],\n",
        "         [False, False, True, False, False, False, True], [False, False, True, False, False, False, False],\n",
        "         [False, False, False, True, False, False, True], [False, False, False, True, False, False, False],\n",
        "         [False, False, False, False, True, False, True], [False, False, False, False, True, False, False],\n",
        "         [False, False, False, False, False, True, True], [False, False, False, False, False, True, False]]\n",
        "\n",
        "#You could also replicate the above list by making two matrices which are the 6x6 identity matrixa concatenated with a 7th column which is\n",
        "#1 for first_layer_pass = True and 0 for first_layer_pass = False and check the conditions of 1 or 0 instead of True or False\n",
        "\n",
        "norm_iter = 0\n",
        "\n",
        "for norm in norms:\n",
        "  norm_iter += 1\n",
        "  q_val_list = []\n",
        "  ground_truth_hits = []\n",
        "  any_motif_hits = []\n",
        "  mean_auroc = []\n",
        "  mean_aupr = []\n",
        "  aurocy = []\n",
        "  aupry = []\n",
        "  #Change to i in range(5) for the full training loop; in range(1) is just to expedite the testing process\n",
        "  for i in range(5):\n",
        "    modely = cnn4((L,A), 12, 2, norm[0], norm[1], norm[2], norm[3], norm[4], norm[5], norm[6])\n",
        "    \n",
        "    auroc = keras.metrics.AUC(curve='ROC', name='auroc')\n",
        "    aupr = keras.metrics.AUC(curve='PR', name='aupr')\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss = keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0)\n",
        "    modely.compile(optimizer=optimizer,\n",
        "                    loss=loss,\n",
        "                    metrics=[auroc, aupr])\n",
        "\n",
        "    keras.backend.clear_session() \n",
        "    \n",
        "    #Train the model\n",
        "    # early stopping callback\n",
        "    es_callback = keras.callbacks.EarlyStopping(monitor='val_aupr', #'val_aupr',#\n",
        "                                                patience=12, \n",
        "                                                verbose=1, \n",
        "                                                mode='max', \n",
        "                                                restore_best_weights=False)\n",
        "    # reduce learning rate callback\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_aupr', \n",
        "                                                    factor=0.2,\n",
        "                                                    patience=4, \n",
        "                                                    min_lr=1e-7,\n",
        "                                                    mode='max',\n",
        "                                                    verbose=1) \n",
        "\n",
        "    # train model\n",
        "    history = modely.fit(x_train, y_train, \n",
        "                        epochs=100,\n",
        "                        batch_size=100, \n",
        "                        shuffle=True,\n",
        "                        validation_data=(x_valid, y_valid), \n",
        "                        callbacks=[es_callback, reduce_lr])\n",
        "    \n",
        "\n",
        "    # evaluate model on test set\n",
        "    results = modely.evaluate(x_test, y_test, batch_size=512)  \n",
        "\n",
        "    #auroc = []\n",
        "    #aupr = []\n",
        "    aurocy.append(results[1])\n",
        "    aupry.append(results[2])\n",
        "\n",
        "    if i!= 4:\n",
        "      mean_auroc.append(0)\n",
        "      mean_aupr.append(0)\n",
        "    elif i==4:\n",
        "      meany_auroc = 0\n",
        "      for i in aurocy:\n",
        "        meany_auroc += i\n",
        "\n",
        "      meany_auroc = meany_auroc/len(aurocy)\n",
        "\n",
        "      meany_aupr = 0\n",
        "      for i in aupry:\n",
        "        meany_aupr += i\n",
        "\n",
        "      meany_aupr = meany_aupr/len(aupry)\n",
        "\n",
        "\n",
        "      mean_auroc.append(meany_auroc)\n",
        "      mean_aupr.append(meany_aupr)\n",
        "    #aupr = [results[2]]\n",
        "\n",
        "    \n",
        "\n",
        "    #Create a  pandas dataframe to store our metrics in\n",
        "    #df = DataFrame({'auroc': auroc, 'aupr': aupr})\n",
        "    #df\n",
        "\n",
        "    #Write this dataframe to either a csv or excel file\n",
        "    #Note that this does not work at the moment and I am unsre why\n",
        "    #%%file 'NormData.xlsx'\n",
        "    #!type df.to_excel('NormData.xlsx', sheet_name='sheet1', index=False)\n",
        "    #df.to_csv('CNN4_Norm_Results.csv', index=False)\n",
        "\n",
        "\n",
        "    #W4 is the position prbability matrix\n",
        "    W4 = moana.filter_activations(x_test, modely, layer=2, window=20, threshold=0.5)\n",
        "    #print('W4', W4)\n",
        "\n",
        "    fig = plt.figure(figsize=(20,4))\n",
        "    W_df = impress.plot_filters(W4, fig, num_cols=8, alphabet='ACGT')\n",
        "\n",
        "\n",
        "    # clip filters\n",
        "    W_clipped = moana.clip_filters(W4, threshold=0.5, pad=3)\n",
        "\n",
        "    #Need to find a way to find the path to my google drive to save each meme file for each iteration of the model to a different filename in google drive\n",
        "    #i.e: cnn4_meme_iter1, cnn4_meme_iter2, cnn4_meme_iter3, cnn4_meme_iter4, cnn4_meme_iter5\n",
        "    #pathtogoogledrive = os.path.join()\n",
        "\n",
        "    # generate meme file\n",
        "    meme_name = 'cnn4_filters_+' + 'norm_iter(' + str(norm_iter) + ')_' + 'training_iter(' + str(i) + ').meme'\n",
        "    moana.meme_generate(W_clipped, output_file= meme_name, prefix='filter')\n",
        "\n",
        "    #from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    #from googleapiclient.discovery import build\n",
        "    drive_service = build('drive', 'v3')\n",
        "\n",
        "    with open('/tmp/' + meme_name, 'a') as f:\n",
        "      f.write('')\n",
        "\n",
        "    print('/tmp/' + meme_name + ' contains:')\n",
        "    !cat /tmp/to_upload.meme\n",
        "\n",
        "    #from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "    file_metadata = {\n",
        "      'name': meme_name,\n",
        "      'mimeType': 'text/plain'\n",
        "    }\n",
        "    media = MediaFileUpload(meme_name, \n",
        "                            mimetype='text/plain',\n",
        "                            resumable=True)\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                          media_body=media,\n",
        "                                          fields='id').execute()\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    '''\n",
        "    #Now we need to upload the weights of our model to the google drive\n",
        "    \n",
        "    weights = []\n",
        "\n",
        "    #auth.authenticate_user()\n",
        "  \n",
        "    #drive_service = build('drive', 'v3')\n",
        "\n",
        "\n",
        "    weights_name = 'cnn4_filters_bnbefore_fptrue_weights' + str(i) + '.h5'\n",
        "    weights_name = 'cnn4_filters_' + str(norm_iter) + str(i) + '.h5'\n",
        "\n",
        "    for layer in modely.layers:\n",
        "      weights.append(layer.get_weights()) # list of numpy arrays\\\n",
        "\n",
        "    outfile = open(weights_name, 'wb')\n",
        "\n",
        "    pickle.dump(weights, outfile)\n",
        "\n",
        "    outfile.close()\n",
        "\n",
        " \n",
        "    with open('/tmp/' + weights_name, 'a') as f:\n",
        "      f.write('')\n",
        "\n",
        "    print('/tmp/' + weights_name + ' contains:')\n",
        "    !cat /tmp/to_upload.csv\n",
        "\n",
        "\n",
        "    file_metadatay = {\n",
        "      'name': weights_name,\n",
        "      'mimeType': 'application/vnd.google-apps.file'\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "    mediay = MediaFileUpload(weights_name,\n",
        "                            resumable=True\n",
        "  )\n",
        "    createdy = drive_service.files().create(body=file_metadatay,\n",
        "                                          media_body=mediay,\n",
        "                                          fields='id').execute()\n",
        "    print('File ID: {}'.format(createdy.get('id')))\n",
        "    '''\n",
        "    weights_name = 'cnn4_filters_' + 'norm_iter(' + str(norm_iter) + ')_' + 'training_iter(' + str(i) + ').h5'\n",
        "    # Save the entire model as a SavedModel.\n",
        "    #!mkdir -p saved_model\n",
        "    #modely.save(weights_name) \n",
        "\n",
        "    #!ls saved_model\n",
        "\n",
        "    # Contains an assets folder, saved_model.pb, and variables folder.\n",
        "    #!ls saved_model/my_model\n",
        "\n",
        "    outfile = open(weights_name, 'wb')\n",
        "\n",
        "    pickle.dump(modely.save_weights(weights_name), outfile)\n",
        "\n",
        "    outfile.close()\n",
        "    \n",
        "    file_metadatay = {\n",
        "        'name': weights_name\n",
        "      }\n",
        "\n",
        "      \n",
        "\n",
        "    mediay = MediaFileUpload(weights_name,\n",
        "                              resumable=True\n",
        "    )\n",
        "    createdy = drive_service.files().create(body=file_metadatay,\n",
        "                                            media_body=mediay,\n",
        "                                            fields='id').execute()\n",
        "    print('File ID: {}'.format(createdy.get('id')))\n",
        "\n",
        "\n",
        "    #Perform ground truth analysis\n",
        "    motif_path = meme_name\n",
        "    output_path = 'cnn4_filters'\n",
        "    jaspar_path = 'JASPAR_CORE_2016_vertebrates.meme'\n",
        "    stdout, stderr = moana.tomtom(motif_path, jaspar_path, output_path, evalue=True, thresh=0.5, dist='pearson', png=None, tomtom_path='./src/tomtom')\n",
        "\n",
        "\n",
        "    num_filters = moana.count_meme_entries(motif_path)\n",
        "\n",
        "\n",
        "    #CNN-4 results\n",
        "    file_path = 'cnn4_filters/tomtom.tsv'\n",
        "    match_fraction4, match_any4, filter_match4, filter_qvalue4, min_qvalue4, num_counts4 \\\n",
        "            = evaluate.motif_comparison_synthetic_dataset(file_path, num_filters=num_filters)\n",
        "    #print(\"CNN-4 analysis\")\n",
        "    #print(\"Fraction of hits to ground truth motifs: %.3f\"%(match_fraction4))\n",
        "    #print(\"Fraction of hits to any motif: %.3f\"%(match_any4))\n",
        "    #print('Hits to ground truth motif for each filter:')\n",
        "    #print(filter_match4)\n",
        "    #print('min q value: ', min_qvalue4)\n",
        "\n",
        "    q_val_list.append(min_qvalue4)\n",
        "    ground_truth_hits.append(match_fraction4)\n",
        "    any_motif_hits.append(match_any4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Calculate the mean aupr and auroc\n",
        "  #mean_auroc = []\n",
        "  #mean_aupr = []\n",
        "  '''\n",
        "\n",
        "  meany_auroc = 0\n",
        "  for i in aurocy:\n",
        "    meany_auroc += i\n",
        "\n",
        "  meany_auroc = meany_auroc/len(aurocy)\n",
        "\n",
        "  meany_aupr = 0\n",
        "  for i in aupry:\n",
        "    meany_aupr += i\n",
        "\n",
        "  meany_aupr = meany_aupr/len(aupry)\n",
        "\n",
        "\n",
        "  mean_auroc.append(meany_auroc)\n",
        "  mean_aupr.append(meany_aupr)\n",
        "  '''\n",
        "\n",
        "  panda_name = 'CNN4_Norm_Results' + 'norm_iter(' + str(norm_iter) + ')_' + 'training_iter(' + str(i) + '.csv'\n",
        "\n",
        "  df = pd.DataFrame(list(zip(aurocy, aupry, mean_auroc, mean_aupr, any_motif_hits, ground_truth_hits, q_val_list)),\n",
        "               columns =['auroc', 'aupr', 'mean_auroc', 'mean_aupr', 'any motif hits', 'ground truth hits', 'min q vals'])\n",
        "  \n",
        "  df.to_csv(panda_name, index=False)\n",
        "  \n",
        "  with open('/tmp/' + panda_name, 'a') as f:\n",
        "      f.write('')\n",
        "\n",
        "  print('/tmp/' + panda_name + ' contains:')\n",
        "  !cat /tmp/to_upload.csv\n",
        "\n",
        "  file_metadata = {\n",
        "    'name': panda_name\n",
        "  }\n",
        "  media = MediaFileUpload(panda_name, \n",
        "                          resumable=True)\n",
        "  \n",
        "  created = drive_service.files().create(body=file_metadata,\n",
        "                                        media_body=media,\n",
        "                                        fields='id').execute()\n",
        "  print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "\n",
        "\n",
        "  #df\n",
        "\n",
        "\n"
      ]
    }
  ]
}